{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66476d93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f202e596",
   "metadata": {},
   "source": [
    "# QA Bot\n",
    "\n",
    "## Task 0\n",
    "\n",
    "Write a function `def question_answer(question, reference)`: that finds a snippet of text within a reference document to answer a question:\n",
    "\n",
    "- `question` is a string containing the question to answer\n",
    "\n",
    "\n",
    "- `reference` is a string containing the reference document from which to find the answer\n",
    "\n",
    "\n",
    "- `Returns`: a string containing the answer\n",
    "\n",
    "\n",
    "- If no answer is found, return `None`\n",
    "\n",
    "\n",
    "- Your function should use the `bert-uncased-tf2-qa` model from the `tensorflow-hub` library\n",
    "\n",
    "\n",
    "- Your function should use the pre-trained `BertTokenizer`, `bert-large-uncased-whole-word-masking-finetuned-squad`, from the transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84e008ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\evisp\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Defines function that finds a snippet of text within a reference document\n",
    "to answer a question\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "def question_answer(question, reference):\n",
    "    \"\"\"\n",
    "    Finds a snippet of text within a reference document to answer a question\n",
    "    \"\"\"\n",
    "    \n",
    "    # Specialized for the SQuAD (Stanford Question Answering Dataset) task\n",
    "    tokenizer = BertTokenizer.from_pretrained(\n",
    "        'bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "    \n",
    "    # Predict the start and end positions of an answer in a text passage\n",
    "    model = hub.load(\"https://tfhub.dev/see--/bert-uncased-tf2-qa/1\")\n",
    "\n",
    "    # Breaking the text down into smaller units (tokens) that the model can understand\n",
    "    quest_tokens = tokenizer.tokenize(question)\n",
    "    refer_tokens = tokenizer.tokenize(reference)\n",
    "\n",
    "    # Preparation of Input Sequence. Add special tokens to include \"classification\" and \"separator\"\n",
    "    tokens = ['[CLS]'] + quest_tokens + ['[SEP]'] + refer_tokens + ['[SEP]']\n",
    "\n",
    "    # The tokens are converted into numerical IDs\n",
    "    input_word_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    \n",
    "    # A list of 1 indicates the presence of a token\n",
    "    # Used to differentiate between tokens and padding\n",
    "    input_mask = [1] * len(input_word_ids)\n",
    "    \n",
    "    # 0 for question segment, 1 for reference segments\n",
    "    input_type_ids = [0] * (1 + len(quest_tokens) + 1) + [1] * (len(refer_tokens) + 1)\n",
    "\n",
    "    # Convert the input data to TF tensors, with additional batch\n",
    "    # Used to provide the data to BERT model\n",
    "    input_word_ids, input_mask, input_type_ids = map(\n",
    "        lambda t: tf.expand_dims(\n",
    "            tf.convert_to_tensor(t, dtype=tf.int32), 0),\n",
    "        (input_word_ids, input_mask, input_type_ids))\n",
    "\n",
    "    # call the bert model\n",
    "    outputs = model([input_word_ids, input_mask, input_type_ids])\n",
    "\n",
    "    # find the positions of the start and end \n",
    "    # of predicted answer span in model outputs\n",
    "    # output[0] represents the logits (predictions) for the start position of the answer\n",
    "    # output[0][0] indexes into the first (and only) batch\n",
    "    # output[0][0][1] identifies the index of the maximum value in the sliced tensor.\n",
    "    short_start = tf.argmax(outputs[0][0][1:]) + 1\n",
    "    short_end = tf.argmax(outputs[1][0][1:]) + 1\n",
    "    answer_tokens = tokens[short_start: short_end + 1]\n",
    "    answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "\n",
    "    if answer == None or answer == \"\" or question in answer:\n",
    "        return None\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf5b49c",
   "metadata": {},
   "source": [
    "### Main (Test) File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d31cfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your professionalism will be equally as important as your actual engineering abilities . we want to make sure that our students leave school not only with great technical skills , but also the professional skills to help them throughout their careers . each trimester , the students are given a baseline professional track score of 100 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nWhat is Professional Track --> ProfessionalTrack.md\\nWhat is speaker of the day? --> SpeakeroftheDay.md\\nCan I study and work at the same time? --> Specializations-FAQ1.md\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('ZendeskArticles/ProfessionalTrack.md') as f:\n",
    "    reference = f.read()\n",
    "\n",
    "print(question_answer('What is Professional Track', reference))\n",
    "\n",
    "\"\"\"\n",
    "What is Professional Track --> ProfessionalTrack.md\n",
    "What is speaker of the day? --> SpeakeroftheDay.md\n",
    "Can I study and work at the same time? --> Specializations-FAQ1.md\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb88804",
   "metadata": {},
   "source": [
    "## Task 1. Create the Loop\n",
    "\n",
    "Create a script that takes in input from the user with the prompt `Q:` and prints `A:` as a response. If the user inputs `exit`, `quit`, `goodbye`, or `bye`, case insensitive, print `A: Goodbye and exit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e090522a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Hello\n",
      "A:\n",
      "Q: Goodbye\n",
      "A: Goodbye\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Script that takes in user input with the prompt 'Q:' and\n",
    "prints 'A:' as the response.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    while (True):\n",
    "        user_input = input(\"Q: \")\n",
    "        user_input = user_input.lower()\n",
    "        if user_input == 'exit' or user_input == 'quit' \\\n",
    "           or user_input == 'goodbye' or user_input == 'bye':\n",
    "            print(\"A: Goodbye\")\n",
    "            break\n",
    "        print(\"A:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d4f078",
   "metadata": {},
   "source": [
    "## 2. Answer Questions\n",
    "\n",
    "Based on the previous tasks, write a function `def answer_loop(reference)`: that answers questions from a reference text:\n",
    "\n",
    "- `reference` is the reference text\n",
    "\n",
    "\n",
    "- If the answer cannot be found in the reference text, respond with `Sorry, I do not understand your question`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d088dfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Defines function that answers questions from reference text on loop\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "def answer_loop(reference):\n",
    "    \"\"\"\n",
    "    Answers questions from a reference text on loop\n",
    "    \"\"\"\n",
    "    while (1):\n",
    "        user_input = input(\"Q: \")\n",
    "        user_input = user_input.lower()\n",
    "        if user_input == 'exit' or user_input == 'quit' \\\n",
    "           or user_input == 'goodbye' or user_input == 'bye':\n",
    "            print(\"A: Goodbye\")\n",
    "            break\n",
    "        answer = question_answer(user_input, reference)\n",
    "        if answer is None:\n",
    "            print(\"A: Sorry, I do not understand your question.\")\n",
    "        else:\n",
    "            print(\"A: \", answer)\n",
    "\n",
    "\n",
    "def question_answer(question, reference):\n",
    "    \"\"\"\n",
    "    Finds a snippet of text within a reference document to answer a question\n",
    "    \"\"\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(\n",
    "        'bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "    model = hub.load(\"https://tfhub.dev/see--/bert-uncased-tf2-qa/1\")\n",
    "\n",
    "    quest_tokens = tokenizer.tokenize(question)\n",
    "    refer_tokens = tokenizer.tokenize(reference)\n",
    "\n",
    "    tokens = ['[CLS]'] + quest_tokens + ['[SEP]'] + refer_tokens + ['[SEP]']\n",
    "\n",
    "    input_word_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_mask = [1] * len(input_word_ids)\n",
    "    input_type_ids = [0] * (\n",
    "        1 + len(quest_tokens) + 1) + [1] * (len(refer_tokens) + 1)\n",
    "\n",
    "    input_word_ids, input_mask, input_type_ids = map(\n",
    "        lambda t: tf.expand_dims(\n",
    "            tf.convert_to_tensor(t, dtype=tf.int32), 0),\n",
    "        (input_word_ids, input_mask, input_type_ids))\n",
    "\n",
    "    outputs = model([input_word_ids, input_mask, input_type_ids])\n",
    "\n",
    "    short_start = tf.argmax(outputs[0][0][1:]) + 1\n",
    "    short_end = tf.argmax(outputs[1][0][1:]) + 1\n",
    "    answer_tokens = tokens[short_start: short_end + 1]\n",
    "    answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "\n",
    "    if answer == None or answer == \"\" or question in answer:\n",
    "        return None\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf19381",
   "metadata": {},
   "source": [
    "### Main (Test) File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c4a2f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: When are PLDs?\n",
      "A:  on - site days from 9 : 00 am to 3 : 00 pm\n",
      "Q: Goodbye\n",
      "A: Goodbye\n"
     ]
    }
   ],
   "source": [
    "with open('ZendeskArticles/PeerLearningDays.md') as f:\n",
    "    reference = f.read()\n",
    "\n",
    "answer_loop(reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2686d2d1",
   "metadata": {},
   "source": [
    "## 3. Semantic Search\n",
    "\n",
    "Write a function `def semantic_search(corpus_path, sentence)`: that performs semantic search on a corpus of documents:\n",
    "\n",
    "- `corpus_path` is the path to the corpus of reference documents on which to perform semantic search\n",
    "\n",
    "\n",
    "- `sentence` is the sentence from which to perform semantic search\n",
    "\n",
    "\n",
    "- Returns: the reference text of the document most similar to sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dca6d017",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Defines function that performs semantic search on a corpus of documents\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "\n",
    "def semantic_search(corpus_path, sentence):\n",
    "    \"\"\"\n",
    "    Performs semantic search on a corpus of documents\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Start with a list containing just the sentence that will be compared against the corpus\n",
    "    documents = [sentence]\n",
    "\n",
    "    # 2. Load and Read Documents\n",
    "    for filename in os.listdir(corpus_path):\n",
    "        if filename.endswith(\".md\") is False:\n",
    "            continue\n",
    "        with open(corpus_path + \"/\" + filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            documents.append(f.read())\n",
    "\n",
    "    # 3. Load Pre-trained Model\n",
    "    \"\"\"\n",
    "        The Universal Sentence Encoder (USE) model from TensorFlow Hub encodes sentences into fixed-size embeddings. \n",
    "        This model is designed to produce semantically meaningful vectors that capture the meaning of sentences.\n",
    "    \"\"\"\n",
    "    model = hub.load(\n",
    "        \"https://tfhub.dev/google/universal-sentence-encoder-large/5\")\n",
    "\n",
    "    # Pass the list of documents to the model. This produces embeddings for each document, including the sentence\n",
    "    embeddings = model(documents)\n",
    "\n",
    "    # Compute the similarity between each pair of embeddings using the inner product\n",
    "    correlation = np.inner(embeddings, embeddings)\n",
    "\n",
    "    # Find the Most Similar Document, Identify Most Similar Document\n",
    "    closest = np.argmax(correlation[0, 1:])\n",
    "\n",
    "    # Return the Most Similar Document\n",
    "    similar = documents[closest + 1]\n",
    "\n",
    "    return similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425fbca8",
   "metadata": {},
   "source": [
    "### Main (Test) File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1b9ba59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stand Up is a meeting that takes place daily on campus at the same specified time. It is an opportunity for staff and students to make announcements pertinent to the community. Each stand up will be live-streamed and available for viewing through the intranet. \n",
      "It is mandatory for all students on campus to attend Stand Up. Students who are on campus, but not present in the stand up area, are at risk for a deduction to their professionalism score.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n    When are PLDs?\\n    What is a Stand Up?\\n    Can I study and work at the same time?\\n    Have the Specializations been tested to ensure there aren’t any major bugs or technical discrepancies?\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(semantic_search('ZendeskArticles', 'What is a Stand Up?'))\n",
    "\n",
    "\"\"\"\n",
    "    When are PLDs?\n",
    "    What is a Stand Up?\n",
    "    Can I study and work at the same time?\n",
    "    Have the Specializations been tested to ensure there aren’t any major bugs or technical discrepancies?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbf32b9",
   "metadata": {},
   "source": [
    "## 4. Multi-reference Question Answering\n",
    "\n",
    "Based on the previous tasks, write a function `def question_answer(coprus_path)`: that answers questions from multiple reference texts:\n",
    "\n",
    "\n",
    "- `corpus_path` is the path to the corpus of reference documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "435af431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Defines function that answers questions from multiple reference texts on loop\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "def question_answer(corpus_path):\n",
    "    \"\"\"\n",
    "    Answers questions from multiple reference texts\n",
    "    \"\"\"\n",
    "    while (1):\n",
    "        user_input = input(\"Q: \")\n",
    "        user_input = user_input.lower()\n",
    "        if user_input == 'exit' or user_input == 'quit' \\\n",
    "           or user_input == 'goodbye' or user_input == 'bye':\n",
    "            print(\"A: Goodbye\")\n",
    "            break\n",
    "        reference = semantic_search(corpus_path, user_input)\n",
    "        answer = specific_question_answer(user_input, reference)\n",
    "        if answer is None:\n",
    "            print(\"A: Sorry, I do not understand your question.\")\n",
    "        else:\n",
    "            print(\"A: \", answer)\n",
    "\n",
    "\n",
    "def semantic_search(corpus_path, sentence):\n",
    "    \"\"\"\n",
    "    Performs semantic search on a corpus of documents\n",
    "    \"\"\"\n",
    "    documents = [sentence]\n",
    "\n",
    "    for filename in os.listdir(corpus_path):\n",
    "        if filename.endswith(\".md\") is False:\n",
    "            continue\n",
    "        with open(corpus_path + \"/\" + filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            documents.append(f.read())\n",
    "\n",
    "    model = hub.load(\n",
    "        \"https://tfhub.dev/google/universal-sentence-encoder-large/5\")\n",
    "\n",
    "    embeddings = model(documents)\n",
    "    correlation = np.inner(embeddings, embeddings)\n",
    "    closest = np.argmax(correlation[0, 1:])\n",
    "    similar = documents[closest + 1]\n",
    "\n",
    "    return similar\n",
    "\n",
    "\n",
    "def specific_question_answer(question, reference):\n",
    "    \"\"\"\n",
    "    Finds a snippet of text within a reference document to answer a question\n",
    "    \"\"\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(\n",
    "        'bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "    model = hub.load(\"https://tfhub.dev/see--/bert-uncased-tf2-qa/1\")\n",
    "\n",
    "    quest_tokens = tokenizer.tokenize(question)\n",
    "    refer_tokens = tokenizer.tokenize(reference)\n",
    "\n",
    "    tokens = ['[CLS]'] + quest_tokens + ['[SEP]'] + refer_tokens + ['[SEP]']\n",
    "\n",
    "    input_word_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_mask = [1] * len(input_word_ids)\n",
    "    input_type_ids = [0] * (\n",
    "        1 + len(quest_tokens) + 1) + [1] * (len(refer_tokens) + 1)\n",
    "\n",
    "    input_word_ids, input_mask, input_type_ids = map(\n",
    "        lambda t: tf.expand_dims(\n",
    "            tf.convert_to_tensor(t, dtype=tf.int32), 0),\n",
    "        (input_word_ids, input_mask, input_type_ids))\n",
    "\n",
    "    outputs = model([input_word_ids, input_mask, input_type_ids])\n",
    "\n",
    "    short_start = tf.argmax(outputs[0][0][1:]) + 1\n",
    "    short_end = tf.argmax(outputs[1][0][1:]) + 1\n",
    "    answer_tokens = tokens[short_start: short_end + 1]\n",
    "    answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "\n",
    "    if answer == None or answer == \"\" or question in answer:\n",
    "        return None\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771f3ea1",
   "metadata": {},
   "source": [
    "### Main (Test) File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e475f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Can I study and work at the same time?\n",
      "A: Sorry, I do not understand your question.\n"
     ]
    }
   ],
   "source": [
    "question_answer('ZendeskArticles')\n",
    "\n",
    "\"\"\"\n",
    "    When are PLDs?\n",
    "    What is a Stand Up?\n",
    "    Can I study and work at the same time?\n",
    "    Have the Specializations been tested to ensure there aren’t any major bugs or technical discrepancies?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947cb796",
   "metadata": {},
   "source": [
    "## Happy Coding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
