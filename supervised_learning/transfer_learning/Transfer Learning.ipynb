{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9f48323",
   "metadata": {},
   "source": [
    "# Transfer Learning on Cifar-10 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5655c274",
   "metadata": {},
   "source": [
    "Using a pre-trained model as a starting point for a new, related task. It leverages the learned features from the pre-trained model, improving performance and reducing the need for extensive data and computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43773182",
   "metadata": {},
   "source": [
    "Lets modify the Cifar-10 architecture we have previously built."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d5cb42",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d788a4cc",
   "metadata": {},
   "source": [
    "Importing libraries in programming is essential because it allows us to leverage pre-written code, enabling them to perform complex tasks without reinventing the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "160f01db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 13:00:21.676422: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.16.1\n"
     ]
    }
   ],
   "source": [
    "# Main imports needed\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "print(\"Tensorflow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d155f10",
   "metadata": {},
   "source": [
    "# 2. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e43b458",
   "metadata": {},
   "source": [
    "Loading data is crucial in transfer learning because it forms the foundation upon which pre-trained models are fine-tuned to solve new, specific tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bba2adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape (50000, 32, 32, 3)\n",
      "Test data shape (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# Using keras\n",
    "\n",
    "(x_train_full, y_train_full), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "print(\"Training data shape\", x_train_full.shape)\n",
    "print(\"Test data shape\", x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a77499",
   "metadata": {},
   "source": [
    "# 3. Visualize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245a6ede",
   "metadata": {},
   "source": [
    "Lets get some insight into the dataset, enabling better understanding and decision-making throughout the model adaptation process. Visualizing data helps identify patterns, anomalies, and distributions, ensuring the pre-trained model's assumptions align with the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c264ec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Create a new figure\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Loop over the first 24 images\n",
    "for i in range(24):\n",
    "    # Create a subplot for each image\n",
    "    plt.subplot(4, 6, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(x_train_full[i])\n",
    "\n",
    "    # Set the label as the title\n",
    "    plt.title(class_names[y_train_full[i][0]], fontsize=12)\n",
    "\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a76be7c",
   "metadata": {},
   "source": [
    "# 4. Build Transfer Learning Model\n",
    "\n",
    "## 4.1 Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b724648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Input, Flatten, UpSampling2D\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5205057b",
   "metadata": {},
   "source": [
    "## 4.2 Preprocess Input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7718cca",
   "metadata": {},
   "source": [
    "Ensures that the new dataset is compatible with the pre-trained model's expectations, leading to more accurate and efficient learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42942171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (50000, 32, 32, 3)\n",
      "Test data shape: (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "x_train_full = x_train_full.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Assuming x_train_full and x_test are already loaded as numpy arrays\n",
    "x_train_full = preprocess_input(x_train_full)\n",
    "x_test = preprocess_input(x_test)\n",
    "\n",
    "print(\"Training data shape:\", x_train_full.shape)\n",
    "print(\"Test data shape:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcb48a4",
   "metadata": {},
   "source": [
    "## 4.3 Train, Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914903ed",
   "metadata": {},
   "source": [
    "Lets divide the training set into separate training and validation sets using slicing operations. The training set, x_train_full, is split into two parts, 1 containing the majority of the data and the other containing a smaller portion (5000 samples in this case) which will be used for validation during model training. \n",
    "Corresponding labels are also split into `y_train` and `y_valid`. \n",
    "\n",
    "Additionally, we **convert the class labels from integer format to categorical format** using the `to_categorical` function. This is necessary for categorical classification tasks like CIFAR-10, where each image is assigned one of ten possible categories. \n",
    "\n",
    "Converting the labels to categorical format ensures that they are represented as **one-hot vectors**, which is required by the model during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ade95a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape (45000, 32, 32, 3)\n",
      "Test data shape (10000, 32, 32, 3)\n",
      "Valid data shape (5000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_valid = x_train_full[:-5000], x_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n",
    "\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_valid = to_categorical(y_valid, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "print(\"Training data shape\", x_train.shape)\n",
    "print(\"Test data shape\", x_test.shape)\n",
    "print(\"Valid data shape\", x_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e46fb80",
   "metadata": {},
   "source": [
    "## 4.4 Define Feature Extractor and Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7923c776",
   "metadata": {},
   "source": [
    "The purpose of a feature extractor is to leverage the learned representations from a pre-trained model to extract relevant features from input images.\n",
    "\n",
    "The `feature_extractor` function **takes input tensors representing images and returns the output feature maps** generated by the `ResNet50` model. By setting `include_top=False`, we exclude the fully connected layers at the top of the `ResNet50` architecture, retaining only the convolutional layers. This allows us to **use `ResNet50` as a feature extractor while excluding its classification layers**, which are specific to the `ImageNet` task.\n",
    "\n",
    "Additionally, we freeze the layers of the base `ResNet50` model by setting `layer.trainable = False` for each layer. **Freezing the layers prevents their weights from being updated during training**, ensuring that only the weights of the additional layers we add on top of the base model will be trained. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5441c208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature extractor using ResNet50\n",
    "def feature_extractor(inputs):\n",
    "    base_model = tf.keras.applications.ResNet50(\n",
    "        input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "\n",
    "    # Freeze the layers of the base model\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "        return base_model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3aea3d",
   "metadata": {},
   "source": [
    "Now lets define a classifier function that builds the classification layers on top of the features extracted by the `ResNet50` model. The classifier is responsible for mapping the extracted features to the corresponding class probabilities for the given task.\n",
    "\n",
    "The classifier function **takes the output feature maps from the feature extractor as input and adds several dense layers to perform classification**. First, we *apply a global average pooling layer to reduce the spatial dimensions of the feature maps while retaining important spatial information*. Then, we flatten the pooled feature maps into a `1D` vector to feed into the fully connected layers.\n",
    "\n",
    "Next, **we add two densely connected layers with `ReLU` activation functions**, which introduce non-linearity to the model and allow it to learn complex patterns in the data. These layers progressively reduce the dimensionality of the feature space, capturing increasingly abstract representations of the input data.\n",
    "\n",
    "Finally, **we add a dense output layer with `softmax` activation**, consisting of `10` units corresponding to the `10` classes in the `CIFAR-10` dataset. The softmax function normalizes the output probabilities, ensuring that they sum up to 1 and represent the predicted probabilities for each class. The name \"classification\" is assigned to this layer for easy identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4751c149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(inputs):\n",
    "    x = GlobalAveragePooling2D()(inputs)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dense(10, activation='softmax', name=\"classification\")(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710e93df",
   "metadata": {},
   "source": [
    "## Defining the Final Model\n",
    "In this step, we define the final model by integrating the feature extraction and classification components. The final model takes input tensors representing images and produces output predictions for the given task.\n",
    "\n",
    "The `final_model` function begins by **upsampling the input images using the `UpSampling2D` layer**. This step increases the spatial dimensions of the images to match the input size expected by the `ResNet50` model. By resizing the images to a size of `(224, 224)`, we ensure compatibility with the input shape required by the pre-trained `ResNet50` architecture.\n",
    "\n",
    "Next, **the resized images are passed through the feature extractor**, which extracts relevant features from the input images using the pre-trained `ResNet50` model. The feature extractor leverages the learned representations from the `ResNet50` architecture to capture meaningful patterns and characteristics present in the images.\n",
    "\n",
    "**The extracted features are then fed into the classifier**, which consists of several densely connected layers followed by a softmax output layer. The classifier processes the extracted features and generates class probabilities for each input image, indicating the likelihood of belonging to each of the predefined classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e31fc110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_model(inputs):\n",
    "    resize = UpSampling2D(size=(7,7))(inputs)\n",
    "    resnet_fe = feature_extractor(resize)\n",
    "    classification_output = classifier(resnet_fe)\n",
    "    \n",
    "    return classification_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a833b25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model():\n",
    "    inputs = Input(shape=(32, 32,3))\n",
    "    classification_output = final_model(inputs)\n",
    "    model = Model(inputs=inputs, outputs=classification_output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9dde7c",
   "metadata": {},
   "source": [
    "## Creating and Summarizing the Model\n",
    "\n",
    "In this step, we create the neural network model, which defines the model architecture and compiles it with specified optimization parameters, loss function, and evaluation metrics. Once the model is created, we use the summary method to print a concise summary of its architecture. This summary provides key information about the model's structure, including the type and shape of each layer, the number of parameters, and the output shape of each layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e057823e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ up_sampling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,098,176</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ classification (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,130</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ up_sampling2d (\u001b[38;5;33mUpSampling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │    \u001b[38;5;34m23,587,712\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │     \u001b[38;5;34m2,098,176\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m524,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ classification (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m5,130\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,215,818</span> (100.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m26,215,818\u001b[0m (100.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,162,698</span> (99.80 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m26,162,698\u001b[0m (99.80 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">53,120</span> (207.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m53,120\u001b[0m (207.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = compile_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ee784d",
   "metadata": {},
   "source": [
    "## Training the Model with Early Stopping\n",
    "\n",
    "Here, we employ the early stopping technique by defining an early stopping callback, which monitors the validation loss during training and halts the training process if the validation loss does not improve for a specified number of epochs (patience). The `restore_best_weights=True` argument ensures that the model's weights are reverted to the configuration yielding the lowest validation loss when training concludes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fb5faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m   6/1407\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:37:11\u001b[0m 9s/step - accuracy: 0.1647 - loss: 3.2323"
     ]
    }
   ],
   "source": [
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=20, batch_size=32, validation_data=(x_valid, y_valid), callbacks=[early_stopping])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
